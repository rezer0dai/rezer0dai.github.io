---
title: "Experimental techniques for Reinforcement Learning"
tags:
  - rewheeler
  - reinforcement learning
  - ai
  - research
---

{% include toc %}

# Project REWheeler

Replay Buffer oriented!

## HER : request for research
*Hindsight Experience Replay (**HER**)* : reinforcement learning algorithm that can learn from failure, which can learn successful policies on robotics problems from only sparse rewards.
<br/>In my words even if agent fails to achieve goal (goal-a), it achieves somethings. And that something (goal-b) will be not scattered, but will be evaluated with respect to policy and recorded state-action pairs we taken to achieve that goal-b, but this time our objective will be not originally intended to reach goal-a but goal-b instead. This way we can introduce more closely related learning signal to goal based environment, while avoiding overcomplicated hand engineered reward functions (example 1/0 signal ~ goal was achieved or not).

However this technique has some potential limitations. For example we can not simply use n-step estimators when it comes to [td-error](http://boris-belousov.net/2017/08/10/td-advantage-bellman/), which seeks balance between bias vs variance. This and some others improvements was highlighted at HER specific [OpenAi Rquest for research](https://openai.com/blog/ingredients-for-robotics-research/#requestsforresearchheredition) and i am trying to address some of those in this post and project itself.

**Why** is there problem with n-step while using HER ? Once we setup new goal and trying to calculate td-error it is OK for 1-step lookahead, as from both current state and following next state we ask current policy to get us new action taken with respect to new goal and comparing its outcomes. 
<br/>However if we try to do it with 2+ (n)state, then those states between current one and last n-state was taken with respect to different goals. It means n-state which was achieved via n-steps from current state is not usable for td-error under different goal, as current policy will unlikely choose same actions for given states but new goal, therefore our replay is useless for this purpose in this way.
<br/>But then, (how) can we introduce back n-state approximation ?
### Floating n-step
At first we can take a look how is TD error calculated :
```python
n_qa = self.ac_target(n_goals, n_states)
td_targets = n_rewards + n_discounts * n_qa
qa = self.ac_explorer(goals, states)
td_error = qa - td_targets
```
It is n-state td-error, but it caculates value for given state not n-step forward from it, utilizing it as n-state approximation is done via further discounting. Well, this may seems obvious, but from this point i did not had any objection to try Floating-N-Step approximation!

*In **floating n-step** every learning loop i select not 'fixed n for state', but k-step one where k is randomly selected from range <1,n>.*
On the other side disadvantage is that it takes more computation power as i need it to recalculate it every time as i want to re-randomize k for learning. However this approach theoretically should lead to better generalization and more efficient use of past experience, as we now are able to use n times more distinct td-errors to learn from one episode.

OK, but what it has in common with HER ? well, HER can be seen as natural floating n-step approximation extension.
As now in floating n step is natural to have k equal to 1 (with probability 1:n), which is exactly case of HER, and rest non 1 equal k steps transitions will be evaluated without HER, while ratio (HER vs k-step) for td-error computation should be subject of hyperparameters tuning.
### N-step GAE
Another important part of reinforcement learning is credit assignment problem. Where there are multiple approaches to tackle it, namely [Rudder](https://arxiv.org/abs/1806.07857) which using LSTM and try to backtrack how to decompose reward based on contribution of particular action states transitions. Where other approach named [GAE](https://arxiv.org/abs/1506.02438) looks at this problem from bit different different angle and using more straightforward approach ( as a note it is worth to check nice article explaining GAE and introducing [Online GAE](http://www.breloff.com/DeepRL-OnlineGAE/) :

{% raw %}
$$
\begin{align}
\delta_t^V = r_t + \gamma V(s_{t+1}) - V(s_t) \\
\hat{A}_t^{(k)} = \sum_{l=0}^{k-1}{\gamma^l \delta_{t+l}^V} \\
\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^\infty{(\gamma\lambda)^l \delta_{t+l}^V}
\end{align}
$$
{% endraw %}

This is how it looks at [code](https://github.com/higgsfield/RL-Adventure-2/blob/master/2.gae.ipynb) :
```python
def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):
    values = values + [next_value]
    gae = 0
    returns = []
    for step in reversed(range(len(rewards))):
        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]
        gae = delta + gamma * tau * masks[step] * gae
        returns.insert(0, gae + values[step])
    return returns
```
When i look from my perspective, neural net have lot of freedom to assign reward by themselves, as there is discounted subtraction of Q-values from different states, where neural net can 'tune' it by itself and figure out assignment of credit, while reward coming from environment can be understood just as signal not as an ultimate reward function which you need to handmade and your nn should converge to. And by doing so it may be more natural to use sparse rewards, which will works really only like signal, instead.

And once i look at it from more hacky standpoint, i never saw implementation for n-step gae, it always just recalc for whole episode and in td-error used for two following states (1-step lookahead). But there is little difference between classical n-step approach and gae in a way, that when we look how td-error is computed (see code snippet above) it is basically used for gradients ( in case of DDPG ) only first state and n-state! Rest is in n-step basically sum of discounted reward from state to n-state according to experience in environment, however what it should be for gae ?
<br/>For GAE it is basically, as i look at it, credit assignment value, as from algorithm you can see only subtraction are active as basically every state between first and n-th one are canceling each other, and so only 'weighted' difference between state transitions are left - credit assigned, where reward from environment can really play only role of signal while Q-values between transitions are here the real deal!

### Conclusion
As a result you can use floating-n-step + HER + GAE in one shot, implementation is bit tricky on part with replay buffer, what to store, when and how to recalculate, but it appears to works good in my case. Also it is need to be careful when you can use HER and when not, k-step and HER can not interleave each other in episode, same apply if we rerandomize for replay buffer floating n-step then HER can not be used. More quirks and tricks related you can find [here](https://github.com/rezer0dai/rewheeler/alchemy/agent.py) and [here](https://github.com/rezer0dai/rewheeler/utils/policy.py).

## Cross Algo Cooperation : PPO+DDPG
Reinforcement learning has several algorithms working over policy gradients methods (DDPG, PPO, TRPO, ACER, ..) or value functions (DQN). I prefer working with policy methods as it allows me to work on continuous action spaces more naturally, and i am especially inclined to DDPG due its nice workarounds with backpropagation of value function loss trough policy network and usage of replay buffer. However algorithm like PPO results in more smooth changes of policy per update and are on-policy driven. On the value functions front DQN algorithm achieved big success on variety of Atari games partially due to discrete action space and proved very efficient.

And there i was wondering if i can benefits from *different methods into single learning process*.
<br/>Lets at first see some learning stats of DDPG and PPO respectivelly :
.. as you can see DDPG is somehow fast to understand what it should do from begining, i guess better gradient signal, but as learning progress it can keeps its cool at stagnating on 'local minima'.
.. on the otherside, you can see, that PPO is way too slow to catch up what todo at begining but do quite leaps once it figure it out, and can converge blazingly fast.

so there i start thinking how to combine them to get benefits or both.
- **common replay buffer** :
  lets ppo take steps and ddpg replay from memory (other way around may do the job too, but you need to carefully recalculate probabilities of ppo making that action-state pairs if it comes from DDPG experience, while DDPG does not care about probabilities, so it is more natural fit)
- **synchronization** :
  With sharing experience only in direction ppo->ddpg is one sided cooperation, while it may work too, i would like to enforce more synchronization. So lets synchronize theirs Policy Neural Networks!
  - every X-episodes synchronize ddpg explorer policy network from ppo as pattern, while ppo will get updated its explorer after every episode from ddpg for enhanced exploration ( as ddpg will update faster, but ppo updates are more stable )
  - i did several experiments and the one most (3+ times speedup for learning) payed of, when ddpg did learn only from experience of synced ppo algorithm, and ddpg itself was not actively used for interacting with environment while learning (actually it was used but only 1:10 due to simplicity of implementation), on the other side ppo explorer was synced every episode with ddpg one, and dppg explorer was synced with ppo every second learning round of ppo
  - good results yields also when DDPG actively join exploration in environment, especially it manifest better learning from the beggining but worse at later stages ( inherited more of ddpg specifics in this environment )

### Conclusion
Test results of combining algorithms into one with cooperation seems promising, as we can see on this environment was achieved balance between pros and cons of both algorithms (slower learning at begining but still relatively fast, and good leaps towards mastering environment) and leads to successfully (and 3 times faster) solving environment (no divergence / oscilation, and both algorithms/agents learns how to approach environment even on its own ~ does not exhibit behaviour where one agent behave just randomly while the other making it up for it). I did not tried to combine Q-values with Policy Gradients but i think it is feasible, and can maybe brings some unexpected results, well TODO..

