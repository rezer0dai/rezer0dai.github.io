---
title: "Experimental techniques for Reinforcement Learning"
tags:
  - rewheeler
  - reinforcement learning
  - ai
  - research
---

{% include toc %}

# Project REWheeler

Replay Buffer oriented!

## HER : request for research
  *Hindsight Experience Replay (HER)* : reinforcement learning algorithm that can learn from failure, which can learn successful policies on robotics problems from only sparse rewards.
  However this technique has some potential limitations. For example we can not simply use n-step estimators when it comes to td-error, which seeks balance between bias vs variance. 
  Why is so ? Once we setup new goal and trying to calculate td-error / advantage it is OK for 1-step lookahead, as from both current state and following next state we ask current policy to get us new action taken with respect to new goal. However if we try to do it with 2+ (n) state, then those states between current and n-state was taken with respect to different goals aka n-next_state which was achieved via n-steps from current state is not usable for td-error under different goal. But then, can we introduce back n-state approximation ?
### Floating n-step
At first we can take a look what Q-function ~ critic, really calculates :

despite n-state td-error, it caculates value for given state not n-step forward, utilizing it for n-state approximation is done out of it by discounting. Well, this may seems obvious, but from this point i did not had any objection to try Floating-N-Step approximation, where every learning loop i select not fixed n-state approximation, but k-step one where k is randomly selected from range <1,n>. 
One disadvantage is that it takes more computation power as i need it to recalculate it everytime as i want to rerandomize k for learning. On the other side, it theoreticaly should lead to better generalization and more efficient use of past experience.

OK, but what it has incommon with HER ? well, HER can be seen as natural floating n-step approximation extension.
As now in floating n step is natural to have k equal to 1, which is HER case, and rest non 1 equal k steps will be evaluated without HER, while ratio should be subject of hyperparameters tunning.
### N-step GAE
Another important part of reinforcement learning is credit assignment problem. Where there are multuple approaches to tackle it, namely Rudder which using LSTM and try to backtrack how to devidrle reward based on contribution of particular action states. Where GAE, which is another method, approach it from different angle and using more straightforward approach :
.. as i look at it, neural net have lot of freedom to assign reward by themselves, while reward comming from environment can be understood just as signal not as an ultimate reward function which you need to handmade and your nn should converge to, and by doing so it may be more natural to use sparse rewards instead.
### Conclusion
TODO

## Cross Algo Cooperation : PPO+DDPG
Reinforcement learbing has several algorithms working over policy gradients methods or value functions. I prefer working with policy methods as it allows me to work on continuous action spaces more naturally, and i am especially inclined to DDPG due its nice workarounds with backpropagation of value function loss trough policy network and usage of replay buffer. However algorithm like PPO results in less drastic changes of policy per update and are on-policy driven, and same also DQN algorithm achieved big sucess on variaty of atari games partially due to discrete action space as well and proved very efficient.
And there i was wondering if i can benefits from different methods into single learning process..
Lets at first see some learnubg stats of DDPG and PPO respectivelly :
.. as you can see DDPG is somehow fast to understand what it should do from begining, and its keeps its cool, given good hyperparams for problem, towards end of learning.
.. on the otherside, you can see, that PPO is way too slow to catch up what todo in begining but do quite leaps once it figure it out, and converge blazingly fast

so there i start thinking how to combine them to get benefits or both.
. at first : common replay buffer
- lets ppo take steps and ddpg replay from memory (other way around may do the job too, but you need to carefully recalculate probabilities of ppo making that action-state pairs if it comes from DDPG experience, while DDPG does not care about probabilities)
. but then it is only one sided cooperation, while it may work too, i would like to enforce more synchrobization. So lets synchronize theirs Policy Neural Networks!
- every X-episodes, synchronize ppo explorer policy network, while ppo will get updated its after every episode for enhanced exploration

. i did several experiments and the one most (3+ tines speedup for learning) payed of, when DDPG did learn only from experience of synced PPO algorithm (aka ddog was learned just from replay buffer which consists experience just from PPO, while PPO explorer policy network was synchronized with DDPG after each episode)
. good results yields also when DDPG activelly join exploration in environment

.. by test results this approach seems promissing, as we can see on this environment was achieved balance between pros and cons of both algorithm (slower learning at begining but still relativelly fast, and good leaps towards mastering environment) and leads to sucessfull solvong environment (no divergence / oscilation, and both algorithms/agents learns how to approach environment even on its own ~ does not inhibit behaviour where one agent behave just randomly while the other making it up for it )
### Conclusion
TODO
## MROCS : multiple rewards
One of the most tricky part of RL is engineering reward function which encourage behaviour which is nearly optimal for particular task. This is particulary tricky as the same reward function may encourage human to do task but not necessarily neural net with particular algorithm, and vice versa... Thats where emerged techniques like HER, which gives reward 0/1 based if goal was met. Or novelity/curiosity driven, where reward is assigned based on how unexpected was state-action-nextstate transition. 
On this front i was looking for balance complexity of reward engineering and simplicity of evaluation for neural net. When i was looking at navigation tasks it comes for me most natural way how to achieve it by spliting reward function, instead of euclidian distance or punishing bad behaviour with big penalties, per dimension and assign rewards in HER style (1/0). 
Here similiarity between HER and MROCS is that it assign 1/0 rewards per subtask (dimension distance reached the goal or not) however it differs that trough loss function is pushed mean of all subtask rewards which make it non 1/0 signal anymore. At the end it is also very similiar to complex hand engineered reward functions, however engineering became far more easier, as only thing we need to hand made is to define goals / evaluation points, and rest is up to network and algorithm itself ~ how to maximize its mean in long run.
### Multiple Agents
While for task with fetching object add also reward if object is fetched, 1/0 rewards for moving (actually reaching is better description ~ HER) that object per dimension towards destination. 

Actually my original approach to separate subtask failed, and i come up with MROCS idea on different (multi agent) task after reading MADDPG paper. MADDPG uses very nice trick, as critic is not used for testing in real environment but only during training time, therefore it may have access to more information during training than it have in testing, while actor have access only to information he has access also in testing time. Therefore MADDPG critic access to both agents states, as multiagent environment, however in MROCS i forced critic to output also multiple rewards and maximize their mean as non-cooperation competitive game by single critic, alongside with bit different state-action pairs from replay buffer used for training. Which seems works pretty well at least for Tenis UnityML environment.
From that point i moved to MROCS approach to output multiple values also for navigation, but this time, as described above, with more subtasks sparsely rewarded rather than complex single reward value per agent. And that i used for this REWheeler project evaluated on Reacher UnityML environment.

### Conclusion
move to todo list :
Idea to critic having access during training to environment information what are not accessible during evaluation is quite elegant and have broader potential, however i did not experiment with it throughoutly yet.

## Noisy Networks Heads for Actors
Another interesting challange in reinforcement learning i experimented with is exploration strategy. At first i used OUNoise which is common choice and especially nice once you can run multiple enviroments at once using one actor but having OUNoise per environment emphatise more throughout exploration for agent policy. On the other hand i abandon this approach as i reallized that is not the method i want to use, at least not in general case. Why is that ?
Simple example is MountainCar environment, which is one my favorite as it focus on problem where you need to do quite opposite actions for a moment to be able to achieve goal. And that is not so trivial to achieve, especially with random noise, as RL is bit tricky in a way you must experience good behaviour at least once to learn ~ well it depends on several aspects like how is reward function engineered, type of problem, algorithm used (HER, ..). And when we check mountaincar reward function:

we can see, that it encourages low power use, not anything to help agent to learn from experience, without achieved goal, how to reach goal. Therefore when you are unlucky you may never reach it, as critic can easily overfit to actor current behaviour and learns nothing usefull at the end. 
However ... with OUNoise it wont happen, or at least very unlikely! To see why lets see how random noise will play in this environment alone, totaly random policy, vs OUNoise agent :

.. as we can see random agent as expected get stuck and nothing interesting going on, actually that one i would expect of random noise. But hen we look at OUNoise

.. its totally different cup of tea, it is like expert behaviour, comparsion to random noise. And that said, our agent learns from solved trajectories and it learns only to optimize it trough given reward function, not actually figuring out how to get goal met in first place. 
Therefore, in my opinion, OUNoise can be very effective at some particular tasks but on the others it can bias exploration of agent which is core of learning.
Mostly for those reasons, but also for some others, i preffer noisy networks, which adds gaussian noise to parameters of network and not only to output itself. However i made few quirks to those. 
- Noisy Heads : at first i like at OUNoise, that it can be added per environment for single actor and si enforce better exploration, wheareas with plain noisy networks you have one noise one agent. For this purpose i allowed actor have multiple noise, sigma and .., while sharing same network parameters for non noisy part, and as for target actor network i remove noise for good. This way i have multiple agents with same network but noise.
- partial re-noise :
i renoise only one noise layer for noisy network rather than full. No actual tests on this front, but sounds to me legit and more intuitive.
### Conclusion
TODO

## RNN with goal
As reinforcement learning tasks are sequence process by default, it sound for me more natural to encode state in form of hidden state of RNN rather than expect to be able to fit MDP to single state represenation by hand engineer or bunch of pixels. While RNN may be seen as natural fit for this, it becomes more tricky once you want to use approach like HER, as once you change goal your entire hidden state for given sequence of state-action pairs becomes invalid! But not necessary.. once you force RNN works only over state representation (pixels) and apply goal after RNN output, you can use RNN as intended. Though i see another problem with RNN, particulary usage of hidden state as state representation, where hidden state meaning/representation i changing with learning quite fast, so it need to be recalculated every so often which is performance overhead you need to deal with, likely pararelization is efficient choice as replay buffer can be utilized while learning.
### Conclusion

## Implementation and Features
### N-state
### Encoders
### CrossBuffer
### Seed-ed episode re-run
### Replay cleaning
### Online learning
Monte Carlo style
### Actor-Critic design
Encoders + Actor + Critic; Sync delta; Cliping gradients; advantages boost;
multiple critics;
### Algorithm Heads
### Task wrapper
Multi agents support

## What did not worked as expected
### Critics with different objective
Attention over them ~ though not tested on latest stages
### Good experience
Not tested properly, idea for LunarLander
### Curiosity PrioReplayBuff
TD-error is basically curiosity behind the scenes.
But also TD-error PrioReplayBuff i skiped for testing.
### PPO with CrossBuffer
Reanalyzing experiences
### Encoders double learning

### TODO
parerelism, replay buffer + HER + recalc rnn features all can be done on background, TD-error for ReplayBuffer and also as RewardBonus!

## Reacher environment UnityML
Why this env + intro; How i reconstructed reward and why; task wrapper; multi agents

## Jupyter notebooks
